{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hybrid RNN + Autoencoder Training\n",
        "\n",
        "This notebook implements a hybrid architecture that combines:\n",
        "\n",
        "1. **Pre-trained Autoencoder** for feature extraction\n",
        "2. **RNN** for temporal sequence modeling  \n",
        "3. **Classification head** for age group prediction\n",
        "\n",
        "The approach:\n",
        "- Use autoencoder to extract 64-dimensional features from ECG segments\n",
        "- Feed these features to an RNN for temporal modeling\n",
        "- Classify age groups using the RNN's final hidden state\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Loading](#data-loading)\n",
        "2. [Data Preprocessing](#data-preprocessing)\n",
        "3. [Model Architecture](#model-architecture)\n",
        "4. [Hyperparameter Configuration](#hyperparameter-configuration)\n",
        "5. [Model Training](#model-training)\n",
        "6. [Model Evaluation](#model-evaluation)\n",
        "7. [Visualization and Analysis](#visualization-and-analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load ECG data and create segments for the hybrid model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "# Set device\n",
        "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = \"input/autonomic-aging-a-dataset-to-quantify-changes-of-cardiovascular-autonomic-function-during-healthy-aging-1.0.0\"\n",
        "OUTPUT_DIR = \"ecg_hybrid_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Model configuration\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.3\n",
        "LATENT_DIM = 64  # Autoencoder latent dimension\n",
        "SEQUENCE_LENGTH = 10  # Number of segments per ECG\n",
        "\n",
        "# Training configuration\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LR = 1e-3\n",
        "PATIENCE = 8\n",
        "\n",
        "# Data configuration\n",
        "SIGNAL_LENGTH = 2500\n",
        "SEGMENT_LENGTH = 250  # Each segment is 250 samples\n",
        "NUM_SEGMENTS = SIGNAL_LENGTH // SEGMENT_LENGTH  # 10 segments per ECG\n",
        "\n",
        "print(\"‚úÖ Environment setup completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ECG data function\n",
        "def load_ecg_data():\n",
        "    \"\"\"Load ECG data and create segments for hybrid model\"\"\"\n",
        "    print(\"üîÑ Loading ECG data...\")\n",
        "    \n",
        "    # Load subject info\n",
        "    subject_info_path = os.path.join(DATA_DIR, \"subject-info.csv\")\n",
        "    subject_info = pd.read_csv(subject_info_path)\n",
        "    print(f\"Loaded subject info: {len(subject_info)} subjects\")\n",
        "    \n",
        "    # Get available age groups\n",
        "    age_groups = subject_info['Age_group'].dropna().unique()\n",
        "    print(f\"Age groups available: {sorted(age_groups)}\")\n",
        "    \n",
        "    # Load ECG files using .hea files\n",
        "    import glob\n",
        "    hea_files = glob.glob(os.path.join(DATA_DIR, \"*.hea\"))\n",
        "    hea_files.sort()\n",
        "    \n",
        "    print(f\"Found {len(hea_files)} ECG files\")\n",
        "    \n",
        "    # Load signals and create segments\n",
        "    signals = []\n",
        "    ages = []\n",
        "    \n",
        "    print(\"Loading signals and creating segments...\")\n",
        "    for hea_file in tqdm(hea_files):  # Load all files\n",
        "        try:\n",
        "            # Get record name from .hea file\n",
        "            record_name = os.path.basename(hea_file).replace('.hea', '')\n",
        "            \n",
        "            # Load signal from corresponding .dat file\n",
        "            dat_file = os.path.join(DATA_DIR, record_name + '.dat')\n",
        "            if not os.path.exists(dat_file):\n",
        "                continue\n",
        "                \n",
        "            signal = np.fromfile(dat_file, dtype=np.int16)\n",
        "            \n",
        "            # Normalize signal\n",
        "            signal = signal.astype(np.float32)\n",
        "            signal = (signal - signal.mean()) / (signal.std() + 1e-8)\n",
        "            \n",
        "            # Create segments\n",
        "            segments = []\n",
        "            for i in range(0, len(signal) - SEGMENT_LENGTH + 1, SEGMENT_LENGTH):\n",
        "                segment = signal[i:i + SEGMENT_LENGTH]\n",
        "                if len(segment) == SEGMENT_LENGTH:\n",
        "                    segments.append(segment)\n",
        "            \n",
        "            # Take first 10 segments\n",
        "            segments = segments[:NUM_SEGMENTS]\n",
        "            if len(segments) == NUM_SEGMENTS:\n",
        "                # Get age group using the same logic as wfdb_parser\n",
        "                record_id = record_name.lstrip(\"0\")\n",
        "                if record_id == \"\":\n",
        "                    record_id = \"0\"\n",
        "                \n",
        "                # Check if record_id exists in the DataFrame\n",
        "                if record_id in subject_info['ID'].astype(str).values:\n",
        "                    age_group = subject_info[subject_info['ID'].astype(str) == record_id]['Age_group'].iloc[0]\n",
        "                else:\n",
        "                    age_group = 3  # Default age group\n",
        "                \n",
        "                if not pd.isna(age_group):\n",
        "                    signals.append(np.array(segments))\n",
        "                    ages.append(int(age_group))\n",
        "                    \n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    signals = np.array(signals)\n",
        "    ages = np.array(ages)\n",
        "    \n",
        "    print(f\"Loaded {len(signals)} ECG sequences\")\n",
        "    print(f\"Signal shape: {signals.shape}\")\n",
        "    print(f\"Age groups: {np.unique(ages)}\")\n",
        "    print(f\"Age distribution: {np.bincount(ages)}\")\n",
        "    \n",
        "    return signals, ages\n",
        "\n",
        "# Load the data\n",
        "signals, ages = load_ecg_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Create age groups and prepare data for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create age groups (use ages directly as they're already age groups)\n",
        "age_groups = ages - 1  # Convert to 0-based indexing\n",
        "\n",
        "print(f\"\\nüìä Dataset Statistics:\")\n",
        "print(f\"  ‚Ä¢ Total samples: {len(signals)}\")\n",
        "print(f\"  ‚Ä¢ Age groups: {len(np.unique(age_groups))}\")\n",
        "print(f\"  ‚Ä¢ Class distribution: {np.bincount(age_groups)}\")\n",
        "\n",
        "# Train/validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    signals, age_groups, test_size=0.2, random_state=42, stratify=age_groups\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Train/Validation Split:\")\n",
        "print(f\"  ‚Ä¢ Training samples: {len(X_train)}\")\n",
        "print(f\"  ‚Ä¢ Validation samples: {len(X_val)}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.FloatTensor(X_train), torch.LongTensor(y_train)\n",
        ")\n",
        "val_dataset = torch.utils.data.TensorDataset(\n",
        "    torch.FloatTensor(X_val), torch.LongTensor(y_val)\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"‚úÖ Data preprocessing completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Architecture\n",
        "\n",
        "Define the hybrid RNN + Autoencoder model architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridRNNAutoencoder(nn.Module):\n",
        "    \"\"\"Hybrid RNN + Autoencoder model\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_size, num_layers, latent_dim, n_classes, dropout=0.3):\n",
        "        super(HybridRNNAutoencoder, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        # Autoencoder encoder (pre-trained)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=15, stride=1, padding=7),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            nn.Conv1d(32, 64, kernel_size=15, stride=1, padding=7),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            nn.Conv1d(64, 128, kernel_size=15, stride=1, padding=7),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, latent_dim)\n",
        "        )\n",
        "        \n",
        "        # RNN for temporal modeling\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),  # *2 for bidirectional\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, n_classes)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, segment_length)\n",
        "        batch_size, seq_len, segment_len = x.shape\n",
        "        \n",
        "        # Reshape for autoencoder: (batch_size * sequence_length, 1, segment_length)\n",
        "        x_reshaped = x.view(-1, 1, segment_len)\n",
        "        \n",
        "        # Extract features using autoencoder encoder\n",
        "        features = self.encoder(x_reshaped)  # (batch_size * seq_len, latent_dim)\n",
        "        \n",
        "        # Reshape back to sequence: (batch_size, seq_len, latent_dim)\n",
        "        features = features.view(batch_size, seq_len, self.latent_dim)\n",
        "        \n",
        "        # RNN processing\n",
        "        rnn_out, (hidden, cell) = self.rnn(features)\n",
        "        \n",
        "        # Use the last output for classification\n",
        "        last_output = rnn_out[:, -1, :]  # (batch_size, hidden_size * 2)\n",
        "        \n",
        "        # Classification\n",
        "        output = self.classifier(last_output)\n",
        "        \n",
        "        return output\n",
        "\n",
        "print(\"‚úÖ Hybrid model architecture defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Configuration\n",
        "\n",
        "Set up the model, optimizer, and training parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "n_classes = len(np.unique(age_groups))\n",
        "model = HybridRNNAutoencoder(\n",
        "    input_dim=SEGMENT_LENGTH,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    latent_dim=LATENT_DIM,\n",
        "    n_classes=n_classes,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "print(f\"\\nüèóÔ∏è Model Architecture:\")\n",
        "print(f\"  ‚Ä¢ Input dimension: {SEGMENT_LENGTH}\")\n",
        "print(f\"  ‚Ä¢ Sequence length: {NUM_SEGMENTS}\")\n",
        "print(f\"  ‚Ä¢ Latent dimension: {LATENT_DIM}\")\n",
        "print(f\"  ‚Ä¢ Hidden size: {HIDDEN_SIZE}\")\n",
        "print(f\"  ‚Ä¢ Number of layers: {NUM_LAYERS}\")\n",
        "print(f\"  ‚Ä¢ Number of classes: {n_classes}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"  ‚Ä¢ Total parameters: {total_params:,}\")\n",
        "print(f\"  ‚Ä¢ Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Initialize optimizer and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR*0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"‚úÖ Model and optimizer configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n",
        "\n",
        "Train the hybrid model with early stopping and learning rate scheduling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training functions\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"Train one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "        \n",
        "        # Memory management\n",
        "        if batch_idx % 100 == 0:\n",
        "            torch.mps.empty_cache()\n",
        "    \n",
        "    return total_loss / len(train_loader), correct / total\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate one epoch\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "    \n",
        "    return total_loss / len(val_loader), correct / total\n",
        "\n",
        "print(\"‚úÖ Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "print(f\"\\nüéØ Training Hybrid RNN + Autoencoder Model\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "best_val_acc = 0\n",
        "patience_counter = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step()\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "    print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'best_hybrid_model.pth'))\n",
        "        print(f\"‚úÖ New best model saved! Val Acc: {val_acc:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"üõë Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "    \n",
        "    # Memory management\n",
        "    torch.mps.empty_cache()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\n‚è±Ô∏è Training completed in {training_time:.2f} seconds\")\n",
        "print(f\"üèÜ Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "# Load best model\n",
        "if os.path.exists(f\"{OUTPUT_DIR}/best_hybrid_model.pth\"):\n",
        "    model.load_state_dict(torch.load(f\"{OUTPUT_DIR}/best_hybrid_model.pth\"))\n",
        "    print(\"‚úÖ Loaded best model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Evaluation\n",
        "\n",
        "Evaluate the trained model and generate comprehensive visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate visualizations\n",
        "def generate_visualizations(results, model, test_loader, device):\n",
        "    \"\"\"Generate training visualizations\"\"\"\n",
        "    print(\"\\nüìä Generating visualizations...\")\n",
        "    \n",
        "    # Training curves\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss curves\n",
        "    axes[0, 0].plot(results['train_losses'], label='Train Loss', color='blue')\n",
        "    axes[0, 0].plot(results['val_losses'], label='Val Loss', color='red')\n",
        "    axes[0, 0].set_title('Training and Validation Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "    \n",
        "    # Accuracy curves\n",
        "    axes[0, 1].plot(results['train_accs'], label='Train Acc', color='blue')\n",
        "    axes[0, 1].plot(results['val_accs'], label='Val Acc', color='red')\n",
        "    axes[0, 1].set_title('Training and Validation Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "    \n",
        "    # Test predictions\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('Confusion Matrix')\n",
        "    axes[1, 0].set_xlabel('Predicted')\n",
        "    axes[1, 0].set_ylabel('Actual')\n",
        "    \n",
        "    # Class distribution\n",
        "    unique, counts = np.unique(all_targets, return_counts=True)\n",
        "    axes[1, 1].bar(unique, counts)\n",
        "    axes[1, 1].set_title('Class Distribution')\n",
        "    axes[1, 1].set_xlabel('Age Group')\n",
        "    axes[1, 1].set_ylabel('Count')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'hybrid_training_analysis.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    print(f\"‚úÖ Visualizations saved to {OUTPUT_DIR}/hybrid_training_analysis.png\")\n",
        "\n",
        "# Prepare results for visualization\n",
        "results = {\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'train_accs': train_accs,\n",
        "    'val_accs': val_accs,\n",
        "    'best_val_acc': best_val_acc\n",
        "}\n",
        "\n",
        "# Generate visualizations\n",
        "generate_visualizations(results, model, val_loader, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization and Analysis\n",
        "\n",
        "Final summary and results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "results_summary = {\n",
        "    'model_type': 'Hybrid RNN + Autoencoder',\n",
        "    'total_parameters': total_params,\n",
        "    'trainable_parameters': trainable_params,\n",
        "    'best_val_accuracy': best_val_acc,\n",
        "    'training_time': training_time,\n",
        "    'epochs_trained': len(train_losses),\n",
        "    'final_train_loss': train_losses[-1],\n",
        "    'final_val_loss': val_losses[-1],\n",
        "    'final_train_acc': train_accs[-1],\n",
        "    'final_val_acc': val_accs[-1]\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'hybrid_results.json'), 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nüéâ Training completed!\")\n",
        "print(f\"üìÅ Results saved to: {OUTPUT_DIR}/\")\n",
        "print(f\"üìä Generated files:\")\n",
        "print(f\"  ‚Ä¢ hybrid_training_analysis.png\")\n",
        "print(f\"  ‚Ä¢ hybrid_results.json\")\n",
        "print(f\"  ‚Ä¢ best_hybrid_model.pth\")\n",
        "\n",
        "print(f\"\\nüìà Final Metrics:\")\n",
        "print(f\"  ‚Ä¢ Best validation accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"  ‚Ä¢ Final training loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"  ‚Ä¢ Final validation loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"  ‚Ä¢ Training time: {training_time:.2f} seconds\")\n",
        "print(f\"  ‚Ä¢ Total parameters: {total_params:,}\")\n",
        "print(f\"  ‚Ä¢ Latent dimension: {LATENT_DIM}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
