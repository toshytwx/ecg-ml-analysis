{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECG RNN Training\n",
        "\n",
        "This notebook implements an advanced RNN/LSTM for ECG age group classification with:\n",
        "\n",
        "- **Class imbalance handling**: Focal loss, class weighting, data augmentation\n",
        "- **Advanced architecture**: Bidirectional LSTM, attention mechanisms\n",
        "- **Comprehensive regularization**: Layer normalization, dropout, gradient clipping\n",
        "- **Direct WFDB parsing**: From .dat/.hea files\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Loading](#data-loading)\n",
        "2. [Data Preprocessing](#data-preprocessing)\n",
        "3. [Model Architecture](#model-architecture)\n",
        "4. [Hyperparameter Configuration](#hyperparameter-configuration)\n",
        "5. [Model Training](#model-training)\n",
        "6. [Model Evaluation](#model-evaluation)\n",
        "7. [Visualization and Analysis](#visualization-and-analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load ECG data using WFDB parser with advanced preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import random\n",
        "from collections import Counter\n",
        "import warnings\n",
        "from wfdb_parser import create_wfdb_dataset\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "DATA_PATH = \"/Users/dmytro/Diploma/ecg_ml_analysis/v2/input/autonomic-aging-a-dataset-to-quantify-changes-of-cardiovascular-autonomic-function-during-healthy-aging-1.0.0\"\n",
        "SUBJECT_INFO_CSV = \"/Users/dmytro/Diploma/ecg_ml_analysis/v2/input/autonomic-aging-a-dataset-to-quantify-changes-of-cardiovascular-autonomic-function-during-healthy-aging-1.0.0/subject-info.csv\"\n",
        "OUTPUT_DIR = \"./ecg_rnn_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Dataset parameters\n",
        "WINDOW_SIZE_SEC = 10\n",
        "WINDOW_STEP_SEC = 5\n",
        "PRELOAD_DATA = True  # Enable preloading for faster training (16GB RAM should handle it)\n",
        "\n",
        "# Training parameters\n",
        "RANDOM_STATE = 42\n",
        "TEST_SPLIT = 0.2\n",
        "BATCH_SIZE = 64  # Reduced batch size to prevent memory issues\n",
        "EPOCHS = 20  # Increased for better convergence with improved loss\n",
        "LR = 1e-3  # Optimized learning rate for aligned model\n",
        "GRADIENT_ACCUMULATION_STEPS = 2  # Reduced accumulation steps for faster training\n",
        "\n",
        "# DataLoader optimization parameters\n",
        "NUM_WORKERS = 0  # Disable multiprocessing on macOS to avoid pickle issues\n",
        "PREFETCH_FACTOR = 2  # Prefetch batches for better pipeline\n",
        "\n",
        "# Device detection with M4 Pro GPU support\n",
        "def get_device():\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")  # M4 Pro GPU\n",
        "    elif torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")  # NVIDIA GPU\n",
        "    else:\n",
        "        return torch.device(\"cpu\")  # CPU fallback\n",
        "\n",
        "DEVICE = get_device()\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# M4 Pro GPU optimizations\n",
        "if DEVICE.type == \"mps\":\n",
        "    torch.backends.mps.allow_tf32 = True\n",
        "    torch.backends.mps.allow_fp16 = True\n",
        "    # Set memory watermark ratio to allow more memory usage\n",
        "    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
        "    # Enable mixed precision for faster training\n",
        "    torch.backends.mps.allow_autocast = True\n",
        "    print(\"M4 Pro GPU optimizations enabled with mixed precision and memory watermark disabled\")\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "print(\"‚úÖ Environment setup completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from WFDB files\n",
        "print(\"Loading dataset from WFDB files...\")\n",
        "dataset = create_wfdb_dataset(\n",
        "    data_path=DATA_PATH,\n",
        "    subject_info_csv=SUBJECT_INFO_CSV,\n",
        "    window_size_sec=WINDOW_SIZE_SEC,\n",
        "    window_step_sec=WINDOW_STEP_SEC,\n",
        "    augment=False,  # We'll handle augmentation in the training loop\n",
        "    preload=PRELOAD_DATA\n",
        ")\n",
        "\n",
        "# Train/test split\n",
        "test_size = int(len(dataset) * TEST_SPLIT)\n",
        "train_size = len(dataset) - test_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create a wrapper for the training dataset that enables augmentation\n",
        "class AugmentedDataset:\n",
        "    def __init__(self, base_dataset, indices):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.indices = indices\n",
        "        # Enable augmentation for this dataset\n",
        "        self.base_dataset.augment = True\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.base_dataset[self.indices[idx]]\n",
        "\n",
        "# Create augmented training dataset\n",
        "train_dataset_aug = AugmentedDataset(dataset, train_dataset.indices)\n",
        "\n",
        "n_classes = len(dataset.classes_)\n",
        "n_channels = dataset.max_channels\n",
        "print(f\"Number of classes: {n_classes}, Channels: {n_channels}\")\n",
        "print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
        "print(f\"Augmented train samples: {len(train_dataset_aug)}\")\n",
        "print(f\"Class distribution: {dataset.get_class_distribution()}\")\n",
        "\n",
        "# Debug: Check if indices are valid\n",
        "print(f\"Dataset total samples: {len(dataset)}\")\n",
        "print(f\"Train indices range: {min(train_dataset.indices)} to {max(train_dataset.indices)}\")\n",
        "print(f\"Test indices range: {min(test_dataset.indices)} to {max(test_dataset.indices)}\")\n",
        "\n",
        "print(\"‚úÖ Dataset loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Handle class imbalance and create weighted sampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute class weights for handling imbalance\n",
        "# Get class distribution for weighting\n",
        "all_labels = []\n",
        "for sample in dataset.samples:\n",
        "    all_labels.append(dataset.le.transform([sample['age_group']])[0])\n",
        "\n",
        "class_counts = Counter(all_labels)\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)\n",
        "class_weights = torch.FloatTensor(class_weights).to(DEVICE)\n",
        "\n",
        "print(f\"Class distribution: {dict(class_counts)}\")\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# Use weighted sampler for training (only for training indices)\n",
        "train_labels = [all_labels[i] for i in train_dataset.indices]\n",
        "sample_weights = [class_weights[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "# Create data loaders with optimized settings for maximum performance\n",
        "train_loader = DataLoader(\n",
        "    train_dataset_aug, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sampler=sampler,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False,  # Disable pin_memory on MPS\n",
        "    persistent_workers=False,  # Disable persistent workers with num_workers=0\n",
        "    prefetch_factor=None,  # Disable prefetch with num_workers=0\n",
        "    drop_last=True,  # Ensure consistent batch sizes\n",
        "    collate_fn=None  # Use default collate function for efficiency\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False,  # Disable pin_memory on MPS\n",
        "    persistent_workers=False,  # Disable persistent workers with num_workers=0\n",
        "    prefetch_factor=None,  # Disable prefetch with num_workers=0\n",
        "    drop_last=False,  # Keep all test samples\n",
        "    collate_fn=None  # Use default collate function for efficiency\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Data preprocessing completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Architecture\n",
        "\n",
        "Define the advanced RNN/LSTM architecture with attention mechanisms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced RNN Architecture Components\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    \"\"\"Attention mechanism for RNN outputs.\"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "    def forward(self, rnn_outputs):\n",
        "        # rnn_outputs: (batch_size, seq_len, hidden_size)\n",
        "        attention_weights = F.softmax(self.attention(rnn_outputs), dim=1)\n",
        "        # Weighted sum of RNN outputs\n",
        "        context_vector = torch.sum(attention_weights * rnn_outputs, dim=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class ResidualLSTM(nn.Module):\n",
        "    \"\"\"Residual LSTM block with skip connections.\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "                           bidirectional=True)\n",
        "        \n",
        "        # Skip connection\n",
        "        self.skip_connection = nn.Linear(input_size, hidden_size * 2)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, input_size)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        \n",
        "        # Skip connection\n",
        "        skip = self.skip_connection(x)\n",
        "        # Pad or truncate skip connection to match LSTM output\n",
        "        if skip.size(1) != lstm_out.size(1):\n",
        "            skip = F.interpolate(skip.transpose(1, 2), size=lstm_out.size(1), mode='linear', align_corners=False).transpose(1, 2)\n",
        "        \n",
        "        # Residual connection\n",
        "        output = lstm_out + skip\n",
        "        output = self.layer_norm(output)\n",
        "        \n",
        "        return output\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    \"\"\"Simplified RNN architecture for faster training and lower memory usage.\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, n_classes, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Simple LSTM (no bidirectional for speed and memory)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        \n",
        "        # Simple attention\n",
        "        self.attention = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "        # Enhanced classification head to match CNN capacity\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),  # First layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),  # Second layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, hidden_size // 4),  # Third layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 4, n_classes)  # Output layer\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, channels, seq_len) -> (batch_size, seq_len, channels)\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        \n",
        "        # Simple attention (global average pooling)\n",
        "        attention_weights = F.softmax(self.attention(lstm_out), dim=1)\n",
        "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)\n",
        "        \n",
        "        # Classification\n",
        "        output = self.classifier(context_vector)\n",
        "        \n",
        "        return output\n",
        "\n",
        "print(\"‚úÖ Advanced RNN architecture defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Configuration\n",
        "\n",
        "Set up loss functions, optimizer, and training parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Focal Loss for handling class imbalance\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean', label_smoothing=0.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.label_smoothing = label_smoothing\n",
        "    \n",
        "    def forward(self, inputs, targets):\n",
        "        if self.label_smoothing > 0:\n",
        "            # Apply label smoothing\n",
        "            num_classes = inputs.size(-1)\n",
        "            smooth_targets = torch.zeros_like(inputs)\n",
        "            smooth_targets.fill_(self.label_smoothing / (num_classes - 1))\n",
        "            smooth_targets.scatter_(-1, targets.unsqueeze(-1), 1 - self.label_smoothing)\n",
        "            ce_loss = -(smooth_targets * F.log_softmax(inputs, dim=-1)).sum(dim=-1)\n",
        "        else:\n",
        "            ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        \n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        \n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "# Initialize model\n",
        "hidden_size = 119  # Optimized to match CNN's 141,007 parameters\n",
        "num_layers = 2  # Two layers for optimal parameter count\n",
        "\n",
        "model = SimpleRNN(n_channels, hidden_size, n_classes, num_layers).to(DEVICE)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Advanced optimizer with weight decay\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR*0.01)\n",
        "\n",
        "# Use focal loss optimized for extreme class imbalance with label smoothing\n",
        "criterion = FocalLoss(alpha=2.0, gamma=3.0, label_smoothing=0.1)  # Stronger focus + label smoothing\n",
        "\n",
        "print(\"‚úÖ Model and hyperparameters configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n",
        "\n",
        "Train the RNN model with advanced techniques for handling class imbalance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop with advanced techniques\n",
        "print(f\"\\nüéØ Training RNN Model\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "best_val_acc = 0\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    print(f\"Training epoch {epoch}/{EPOCHS}\")\n",
        "    \n",
        "    # Training\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    for batch_idx, (xb, yb) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)):\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        \n",
        "        # Scale loss for gradient accumulation\n",
        "        loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
        "        loss.backward()\n",
        "        \n",
        "        running_loss += loss.item() * xb.size(0) * GRADIENT_ACCUMULATION_STEPS\n",
        "        preds = out.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "        \n",
        "        # Update weights every GRADIENT_ACCUMULATION_STEPS\n",
        "        if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    # Final gradient update if there are remaining gradients\n",
        "    if len(train_loader) % GRADIENT_ACCUMULATION_STEPS != 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "    train_losses.append(running_loss/total)\n",
        "    train_accs.append(correct/total)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (xb, yb) in enumerate(tqdm(test_loader, desc=\"Validation\", leave=False)):\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "            preds = out.argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    \n",
        "    val_losses.append(running_loss/total)\n",
        "    val_accs.append(correct/total)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_accs[-1] > best_val_acc:\n",
        "        best_val_acc = val_accs[-1]\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'best_rnn_model.pth'))\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    \n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train Acc: {train_accs[-1]:.3f} | Val Acc: {val_accs[-1]:.3f} | Best: {best_val_acc:.3f}\")\n",
        "    \n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, 'best_rnn_model.pth')))\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\n‚è±Ô∏è Training completed in {training_time:.2f} seconds\")\n",
        "print(f\"üèÜ Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "print(\"‚úÖ Model training completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Evaluation\n",
        "\n",
        "Generate comprehensive visualizations and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive evaluation\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "probs_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        out = model(xb)\n",
        "        probs = nn.functional.softmax(out, dim=1)\n",
        "        max_probs, _ = probs.max(1)\n",
        "        probs_list.append(max_probs.cpu().numpy())\n",
        "\n",
        "        preds = out.argmax(dim=1)\n",
        "        y_true.append(yb.cpu().numpy())\n",
        "        y_pred.append(preds.cpu().numpy())\n",
        "\n",
        "y_true = np.concatenate(y_true)\n",
        "y_pred = np.concatenate(y_pred)\n",
        "probs_all = np.concatenate(probs_list)\n",
        "\n",
        "# Comprehensive metrics\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"\\n=== COMPREHENSIVE EVALUATION ===\")\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
        "print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, digits=4, target_names=[f'Age_{c}' for c in dataset.le.classes_]))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"‚úÖ Model evaluation completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# 1) Training curves with learning rate\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes[0,0].plot(train_losses, label=\"Train Loss\")\n",
        "axes[0,0].plot(val_losses, label=\"Val Loss\")\n",
        "axes[0,0].set_xlabel(\"Epoch\"); axes[0,0].set_ylabel(\"Loss\")\n",
        "axes[0,0].set_title(\"Loss Curves\"); axes[0,0].legend(); axes[0,0].grid(True)\n",
        "\n",
        "axes[0,1].plot(train_accs, label=\"Train Acc\")\n",
        "axes[0,1].plot(val_accs, label=\"Val Acc\")\n",
        "axes[0,1].set_xlabel(\"Epoch\"); axes[0,1].set_ylabel(\"Accuracy\")\n",
        "axes[0,1].set_title(\"Accuracy Curves\"); axes[0,1].legend(); axes[0,1].grid(True)\n",
        "\n",
        "# Class distribution\n",
        "class_dist = Counter(y_true)\n",
        "axes[1,0].bar(range(len(class_dist)), list(class_dist.values()))\n",
        "axes[1,0].set_xlabel(\"Age Group\"); axes[1,0].set_ylabel(\"Count\")\n",
        "axes[1,0].set_title(\"Test Set Class Distribution\")\n",
        "axes[1,0].set_xticks(range(len(class_dist)))\n",
        "axes[1,0].set_xticklabels([f'Age_{c}' for c in sorted(class_dist.keys())], rotation=45)\n",
        "\n",
        "# Prediction confidence\n",
        "axes[1,1].hist(probs_all, bins=20, color=\"green\", alpha=0.7)\n",
        "axes[1,1].set_xlabel(\"Max Predicted Probability\")\n",
        "axes[1,1].set_ylabel(\"Count\")\n",
        "axes[1,1].set_title(\"Prediction Confidence Distribution\")\n",
        "axes[1,1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"rnn_training_analysis.png\"))\n",
        "\n",
        "# 2) Enhanced confusion matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[f'Age_{c}' for c in dataset.le.classes_], \n",
        "            yticklabels=[f'Age_{c}' for c in dataset.le.classes_])\n",
        "plt.xlabel(\"Predicted Age Group\"); plt.ylabel(\"True Age Group\")\n",
        "plt.title(f\"RNN Confusion Matrix\\nAccuracy: {acc:.3f} | F1-Macro: {f1_macro:.3f} | F1-Weighted: {f1_weighted:.3f}\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"rnn_confusion_matrix.png\"))\n",
        "\n",
        "print(\"‚úÖ Training analysis and confusion matrix saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization and Analysis\n",
        "\n",
        "Final summary and results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional comprehensive visualizations\n",
        "# 3) Per-class performance analysis\n",
        "per_class_acc = []\n",
        "per_class_f1 = []\n",
        "for i in range(n_classes):\n",
        "    idx = (y_true == i)\n",
        "    if idx.any():\n",
        "        acc_i = (y_pred[idx] == y_true[idx]).mean()\n",
        "        y_true_binary = (y_true == i).astype(int)\n",
        "        y_pred_binary = (y_pred == i).astype(int)\n",
        "        f1_i = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n",
        "    else:\n",
        "        acc_i = 0.0\n",
        "        f1_i = 0.0\n",
        "    per_class_acc.append(acc_i)\n",
        "    per_class_f1.append(f1_i)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "x_pos = range(len(dataset.le.classes_))\n",
        "ax1.bar(x_pos, per_class_acc, color=\"skyblue\", alpha=0.7)\n",
        "ax1.set_xlabel(\"Age Group\"); ax1.set_ylabel(\"Accuracy\")\n",
        "ax1.set_title(\"Per-class Accuracy\")\n",
        "ax1.set_xticks(x_pos)\n",
        "ax1.set_xticklabels([f'Age_{c}' for c in dataset.le.classes_], rotation=45)\n",
        "ax1.grid(axis='y')\n",
        "\n",
        "ax2.bar(x_pos, per_class_f1, color=\"lightcoral\", alpha=0.7)\n",
        "ax2.set_xlabel(\"Age Group\"); ax2.set_ylabel(\"F1-Score\")\n",
        "ax2.set_title(\"Per-class F1-Score\")\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels([f'Age_{c}' for c in dataset.le.classes_], rotation=45)\n",
        "ax2.grid(axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"rnn_per_class_performance.png\"))\n",
        "\n",
        "# 4) Model architecture visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.text(0.1, 0.9, \"Advanced RNN Architecture:\", fontsize=16, fontweight='bold')\n",
        "plt.text(0.1, 0.8, \"‚Ä¢ Bidirectional LSTM with Residual Connections\", fontsize=12)\n",
        "plt.text(0.1, 0.75, \"‚Ä¢ Attention Mechanism for Sequence Modeling\", fontsize=12)\n",
        "plt.text(0.1, 0.7, \"‚Ä¢ Layer Normalization and Dropout\", fontsize=12)\n",
        "plt.text(0.1, 0.65, \"‚Ä¢ Focal Loss for Class Imbalance\", fontsize=12)\n",
        "plt.text(0.1, 0.6, \"‚Ä¢ Weighted Random Sampling\", fontsize=12)\n",
        "plt.text(0.1, 0.55, \"‚Ä¢ Learning Rate Scheduling\", fontsize=12)\n",
        "plt.text(0.1, 0.5, \"‚Ä¢ Early Stopping\", fontsize=12)\n",
        "plt.text(0.1, 0.4, f\"Final Performance:\", fontsize=14, fontweight='bold')\n",
        "plt.text(0.1, 0.35, f\"‚Ä¢ Accuracy: {acc:.4f}\", fontsize=12)\n",
        "plt.text(0.1, 0.3, f\"‚Ä¢ F1-Macro: {f1_macro:.4f}\", fontsize=12)\n",
        "plt.text(0.1, 0.25, f\"‚Ä¢ F1-Weighted: {f1_weighted:.4f}\", fontsize=12)\n",
        "plt.text(0.1, 0.2, f\"‚Ä¢ Classes: {n_classes}\", fontsize=12)\n",
        "plt.text(0.1, 0.15, f\"‚Ä¢ Channels: {n_channels}\", fontsize=12)\n",
        "plt.text(0.1, 0.1, f\"‚Ä¢ Hidden Size: {hidden_size}\", fontsize=12)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"rnn_model_summary.png\"))\n",
        "\n",
        "print(\"‚úÖ Per-class performance and model summary saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"All comprehensive RNN plots saved to:\", OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\nüéâ RNN training completed!\")\n",
        "print(f\"üìÅ Results saved to: {OUTPUT_DIR}/\")\n",
        "print(\"üìä Generated files:\")\n",
        "print(\"  ‚Ä¢ rnn_training_analysis.png\")\n",
        "print(\"  ‚Ä¢ rnn_confusion_matrix.png\")\n",
        "print(\"  ‚Ä¢ rnn_per_class_performance.png\")\n",
        "print(\"  ‚Ä¢ rnn_model_summary.png\")\n",
        "print(\"  ‚Ä¢ best_rnn_model.pth\")\n",
        "\n",
        "print(f\"\\nüìà Final Metrics:\")\n",
        "print(f\"  ‚Ä¢ Test Accuracy: {acc:.4f}\")\n",
        "print(f\"  ‚Ä¢ F1-Score (Macro): {f1_macro:.4f}\")\n",
        "print(f\"  ‚Ä¢ F1-Score (Weighted): {f1_weighted:.4f}\")\n",
        "print(f\"  ‚Ä¢ Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"  ‚Ä¢ Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"  ‚Ä¢ Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  ‚Ä¢ Hidden Size: {hidden_size}\")\n",
        "print(f\"  ‚Ä¢ Number of Classes: {n_classes}\")\n",
        "print(f\"  ‚Ä¢ Number of Channels: {n_channels}\")\n",
        "\n",
        "# Cleanup\n",
        "if DEVICE.type == \"mps\":\n",
        "    torch.mps.empty_cache()\n",
        "print(\"üßπ Cleanup completed\")\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
