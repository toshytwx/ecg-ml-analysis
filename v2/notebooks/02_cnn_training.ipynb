{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECG CNN Training\n",
        "\n",
        "This notebook implements an advanced 1D CNN for ECG age group classification with the following features:\n",
        "\n",
        "- **Class imbalance handling**: Focal loss, class weighting, data augmentation\n",
        "- **Advanced architecture**: ResNet blocks, attention mechanisms\n",
        "- **Comprehensive regularization**: Dropout, batch normalization, gradient clipping\n",
        "- **Direct WFDB parsing**: From .dat/.hea files\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Loading](#data-loading)\n",
        "2. [Data Preprocessing](#data-preprocessing)\n",
        "3. [Model Architecture](#model-architecture)\n",
        "4. [Hyperparameter Configuration](#hyperparameter-configuration)\n",
        "5. [Model Training](#model-training)\n",
        "6. [Model Evaluation](#model-evaluation)\n",
        "7. [Visualization and Analysis](#visualization-and-analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load ECG data using the WFDB parser with advanced preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import random\n",
        "from collections import Counter\n",
        "import warnings\n",
        "from wfdb_parser import create_wfdb_dataset\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "DATA_PATH = \"input/autonomic-aging-a-dataset-to-quantify-changes-of-cardiovascular-autonomic-function-during-healthy-aging-1.0.0\"\n",
        "SUBJECT_INFO_CSV = \"input/autonomic-aging-a-dataset-to-quantify-changes-of-cardiovascular-autonomic-function-during-healthy-aging-1.0.0/subject-info.csv\"\n",
        "OUTPUT_DIR = \"./ecg_cnn_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Dataset parameters\n",
        "WINDOW_SIZE_SEC = 10\n",
        "WINDOW_STEP_SEC = 5\n",
        "PRELOAD_DATA = True  # Optimized for 16GB RAM\n",
        "\n",
        "# Training parameters\n",
        "RANDOM_STATE = 42\n",
        "TEST_SPLIT = 0.2\n",
        "BATCH_SIZE = 128  # Optimized for M4 Pro GPU\n",
        "EPOCHS = 100\n",
        "LR = 1e-3\n",
        "\n",
        "# Device detection with M4 Pro GPU support\n",
        "def get_device():\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")  # M4 Pro GPU\n",
        "    elif torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")  # NVIDIA GPU\n",
        "    else:\n",
        "        return torch.device(\"cpu\")  # CPU fallback\n",
        "\n",
        "DEVICE = get_device()\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# M4 Pro GPU optimizations\n",
        "if DEVICE.type == \"mps\":\n",
        "    torch.backends.mps.allow_tf32 = True\n",
        "    torch.backends.mps.allow_fp16 = True\n",
        "    print(\"M4 Pro GPU optimizations enabled\")\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from WFDB files\n",
        "print(\"Loading dataset from WFDB files...\")\n",
        "dataset = create_wfdb_dataset(\n",
        "    data_path=DATA_PATH,\n",
        "    subject_info_csv=SUBJECT_INFO_CSV,\n",
        "    window_size_sec=WINDOW_SIZE_SEC,\n",
        "    window_step_sec=WINDOW_STEP_SEC,\n",
        "    augment=False,  # We'll handle augmentation in the training loop\n",
        "    preload=PRELOAD_DATA\n",
        ")\n",
        "\n",
        "# Train/test split\n",
        "test_size = int(len(dataset) * TEST_SPLIT)\n",
        "train_size = len(dataset) - test_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create a wrapper for the training dataset that enables augmentation\n",
        "class AugmentedDataset:\n",
        "    def __init__(self, base_dataset, indices):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.indices = indices\n",
        "        # Enable augmentation for this dataset\n",
        "        self.base_dataset.augment = True\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.base_dataset[self.indices[idx]]\n",
        "\n",
        "# Create augmented training dataset\n",
        "train_dataset_aug = AugmentedDataset(dataset, train_dataset.indices)\n",
        "\n",
        "n_classes = len(dataset.classes_)\n",
        "n_channels = dataset.max_channels\n",
        "print(f\"Number of classes: {n_classes}, Channels: {n_channels}\")\n",
        "print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
        "print(f\"Augmented train samples: {len(train_dataset_aug)}\")\n",
        "print(f\"Class distribution: {dataset.get_class_distribution()}\")\n",
        "\n",
        "# Debug: Check if indices are valid\n",
        "print(f\"Dataset total samples: {len(dataset)}\")\n",
        "print(f\"Train indices range: {min(train_dataset.indices)} to {max(train_dataset.indices)}\")\n",
        "print(f\"Test indices range: {min(test_dataset.indices)} to {max(test_dataset.indices)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Handle class imbalance and create weighted sampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute class weights for handling imbalance\n",
        "# Get class distribution for weighting\n",
        "all_labels = []\n",
        "for sample in dataset.samples:\n",
        "    all_labels.append(dataset.le.transform([sample['age_group']])[0])\n",
        "\n",
        "class_counts = Counter(all_labels)\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)\n",
        "class_weights = torch.FloatTensor(class_weights).to(DEVICE)\n",
        "\n",
        "print(f\"Class distribution: {dict(class_counts)}\")\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# Use weighted sampler for training (only for training indices)\n",
        "train_labels = [all_labels[i] for i in train_dataset.indices]\n",
        "sample_weights = [class_weights[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "# Create data loaders (use num_workers=0 to avoid multiprocessing issues on macOS)\n",
        "train_loader = DataLoader(train_dataset_aug, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"‚úÖ Data preprocessing completed\")\n",
        "print(f\"  ‚Ä¢ Training samples: {len(train_dataset_aug)}\")\n",
        "print(f\"  ‚Ä¢ Test samples: {len(test_dataset)}\")\n",
        "print(f\"  ‚Ä¢ Class weights computed: {len(class_weights)} classes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Architecture\n",
        "\n",
        "Define the advanced CNN architecture with ResNet blocks and attention mechanisms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced CNN Architecture Components\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding=kernel_size//2)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        \n",
        "        # Skip connection\n",
        "        self.skip = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, 1, stride),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        residual = self.skip(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Conv1d(channels, channels//4, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(channels//4, channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        att = self.attention(x)\n",
        "        return x * att\n",
        "\n",
        "class AdvancedCNN1D(nn.Module):\n",
        "    def __init__(self, in_channels, n_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Initial convolution\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, 64, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2)\n",
        "        )\n",
        "        \n",
        "        # Residual blocks with attention\n",
        "        self.res_block1 = ResidualBlock(64, 64)\n",
        "        self.attention1 = AttentionBlock(64)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "        \n",
        "        self.res_block2 = ResidualBlock(64, 128, stride=2)\n",
        "        self.attention2 = AttentionBlock(128)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "        \n",
        "        self.res_block3 = ResidualBlock(128, 256, stride=2)\n",
        "        self.attention3 = AttentionBlock(256)\n",
        "        self.pool3 = nn.MaxPool1d(2)\n",
        "        \n",
        "        # Global pooling and classification\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(256, n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x: (B, C, L)\n",
        "        x = self.initial_conv(x)\n",
        "        \n",
        "        x = self.res_block1(x)\n",
        "        x = self.attention1(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = self.res_block2(x)\n",
        "        x = self.attention2(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = self.res_block3(x)\n",
        "        x = self.attention3(x)\n",
        "        x = self.pool3(x)\n",
        "        \n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "print(\"‚úÖ Advanced CNN architecture defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Configuration\n",
        "\n",
        "Set up loss functions, optimizer, and training parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Focal Loss for handling class imbalance\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "    \n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        \n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "# Initialize model\n",
        "model = AdvancedCNN1D(n_channels, n_classes).to(DEVICE)\n",
        "\n",
        "# Advanced optimizer with weight decay\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)\n",
        "\n",
        "# Use focal loss for better handling of class imbalance\n",
        "criterion = FocalLoss(alpha=1, gamma=2)\n",
        "\n",
        "print(\"üìã Hyperparameter Configuration:\")\n",
        "print(f\"  ‚Ä¢ Model: Advanced CNN with ResNet blocks\")\n",
        "print(f\"  ‚Ä¢ Loss function: Focal Loss (alpha=1, gamma=2)\")\n",
        "print(f\"  ‚Ä¢ Optimizer: AdamW (lr={LR}, weight_decay=1e-4)\")\n",
        "print(f\"  ‚Ä¢ Scheduler: ReduceLROnPlateau\")\n",
        "print(f\"  ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  ‚Ä¢ Epochs: {EPOCHS}\")\n",
        "print(f\"  ‚Ä¢ Classes: {n_classes}\")\n",
        "print(f\"  ‚Ä¢ Channels: {n_channels}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"  ‚Ä¢ Total parameters: {total_params:,}\")\n",
        "print(f\"  ‚Ä¢ Model size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n",
        "\n",
        "Train the CNN with advanced techniques including early stopping and gradient clipping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced training with early stopping\n",
        "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "best_val_acc = 0\n",
        "patience = 20\n",
        "patience_counter = 0\n",
        "\n",
        "print(f\"\\nüéØ Training for {EPOCHS} epochs...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    # Training\n",
        "    print(f\"Training epoch {epoch}/{EPOCHS}\")\n",
        "    if DEVICE.type == \"mps\":\n",
        "        print(f\"M4 Pro GPU memory allocated: {torch.mps.current_allocated_memory() / 1024**3:.2f} GB\")\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "        preds = out.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    \n",
        "    train_losses.append(running_loss/total)\n",
        "    train_accs.append(correct/total)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "            preds = out.argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    \n",
        "    val_losses.append(running_loss/total)\n",
        "    val_accs.append(correct/total)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_accs[-1])\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_accs[-1] > best_val_acc:\n",
        "        best_val_acc = val_accs[-1]\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'best_model.pth'))\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    \n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train Acc: {train_accs[-1]:.3f} | Val Acc: {val_accs[-1]:.3f} | Best: {best_val_acc:.3f}\")\n",
        "    \n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, 'best_model.pth')))\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\n‚è±Ô∏è Training completed in {training_time:.2f} seconds\")\n",
        "print(f\"üèÜ Best validation accuracy: {best_val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Evaluation\n",
        "\n",
        "Comprehensive evaluation with detailed metrics and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive evaluation\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "probs_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        out = model(xb)\n",
        "        probs = nn.functional.softmax(out, dim=1)\n",
        "        max_probs, _ = probs.max(1)\n",
        "        probs_list.append(max_probs.cpu().numpy())\n",
        "\n",
        "        preds = out.argmax(dim=1)\n",
        "        y_true.append(yb.cpu().numpy())\n",
        "        y_pred.append(preds.cpu().numpy())\n",
        "\n",
        "y_true = np.concatenate(y_true)\n",
        "y_pred = np.concatenate(y_pred)\n",
        "probs_all = np.concatenate(probs_list)\n",
        "\n",
        "# Comprehensive metrics\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"\\n=== COMPREHENSIVE EVALUATION ===\")\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
        "print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n",
        "print(\"\\nDetailed Classification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, digits=4, target_names=[f'Age_{c}' for c in dataset.le.classes_]))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"‚úÖ Model evaluation completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization and Analysis\n",
        "\n",
        "Generate comprehensive visualizations for training analysis and model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# 1) Training curves with learning rate\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes[0,0].plot(train_losses, label=\"Train Loss\")\n",
        "axes[0,0].plot(val_losses, label=\"Val Loss\")\n",
        "axes[0,0].set_xlabel(\"Epoch\"); axes[0,0].set_ylabel(\"Loss\")\n",
        "axes[0,0].set_title(\"Loss Curves\"); axes[0,0].legend(); axes[0,0].grid(True)\n",
        "\n",
        "axes[0,1].plot(train_accs, label=\"Train Acc\")\n",
        "axes[0,1].plot(val_accs, label=\"Val Acc\")\n",
        "axes[0,1].set_xlabel(\"Epoch\"); axes[0,1].set_ylabel(\"Accuracy\")\n",
        "axes[0,1].set_title(\"Accuracy Curves\"); axes[0,1].legend(); axes[0,1].grid(True)\n",
        "\n",
        "# Class distribution\n",
        "class_dist = Counter(y_true)\n",
        "axes[1,0].bar(range(len(class_dist)), list(class_dist.values()))\n",
        "axes[1,0].set_xlabel(\"Age Group\"); axes[1,0].set_ylabel(\"Count\")\n",
        "axes[1,0].set_title(\"Test Set Class Distribution\")\n",
        "axes[1,0].set_xticks(range(len(class_dist)))\n",
        "axes[1,0].set_xticklabels([f'Age_{c}' for c in sorted(class_dist.keys())], rotation=45)\n",
        "\n",
        "# Prediction confidence\n",
        "axes[1,1].hist(probs_all, bins=20, color=\"green\", alpha=0.7)\n",
        "axes[1,1].set_xlabel(\"Max Predicted Probability\")\n",
        "axes[1,1].set_ylabel(\"Count\")\n",
        "axes[1,1].set_title(\"Prediction Confidence Distribution\")\n",
        "axes[1,1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"training_analysis.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úÖ Training analysis plot saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Enhanced confusion matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[f'Age_{c}' for c in dataset.le.classes_], \n",
        "            yticklabels=[f'Age_{c}' for c in dataset.le.classes_])\n",
        "plt.xlabel(\"Predicted Age Group\"); plt.ylabel(\"True Age Group\")\n",
        "plt.title(f\"Confusion Matrix\\nAccuracy: {acc:.3f} | F1-Macro: {f1_macro:.3f} | F1-Weighted: {f1_weighted:.3f}\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úÖ Confusion matrix saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Per-class performance analysis\n",
        "per_class_acc = []\n",
        "per_class_f1 = []\n",
        "for i in range(n_classes):\n",
        "    idx = (y_true == i)\n",
        "    if idx.any():\n",
        "        acc_i = (y_pred[idx] == y_true[idx]).mean()\n",
        "        # For multiclass, we need to use a different approach for per-class F1\n",
        "        # Create binary labels for this class vs all others\n",
        "        y_true_binary = (y_true == i).astype(int)\n",
        "        y_pred_binary = (y_pred == i).astype(int)\n",
        "        f1_i = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n",
        "    else:\n",
        "        acc_i = 0.0\n",
        "        f1_i = 0.0\n",
        "    per_class_acc.append(acc_i)\n",
        "    per_class_f1.append(f1_i)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "x_pos = range(len(dataset.le.classes_))\n",
        "ax1.bar(x_pos, per_class_acc, color=\"skyblue\", alpha=0.7)\n",
        "ax1.set_xlabel(\"Age Group\"); ax1.set_ylabel(\"Accuracy\")\n",
        "ax1.set_title(\"Per-class Accuracy\")\n",
        "ax1.set_xticks(x_pos)\n",
        "ax1.set_xticklabels([f'Age_{c}' for c in dataset.le.classes_], rotation=45)\n",
        "ax1.grid(axis='y')\n",
        "\n",
        "ax2.bar(x_pos, per_class_f1, color=\"lightcoral\", alpha=0.7)\n",
        "ax2.set_xlabel(\"Age Group\"); ax2.set_ylabel(\"F1-Score\")\n",
        "ax2.set_title(\"Per-class F1-Score\")\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels([f'Age_{c}' for c in dataset.le.classes_], rotation=45)\n",
        "ax2.grid(axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"per_class_performance.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úÖ Per-class performance analysis saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Class imbalance analysis\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# True vs Predicted distribution\n",
        "ax1.hist(y_true, bins=len(dataset.le.classes_), alpha=0.7, label=\"True\", color=\"blue\")\n",
        "ax1.hist(y_pred, bins=len(dataset.le.classes_), alpha=0.7, label=\"Predicted\", color=\"red\")\n",
        "ax1.set_xlabel(\"Age Group\"); ax1.set_ylabel(\"Count\")\n",
        "ax1.set_title(\"True vs Predicted Distribution\")\n",
        "ax1.legend(); ax1.grid(True)\n",
        "\n",
        "# Class weights visualization\n",
        "class_weights_np = class_weights.cpu().numpy()\n",
        "ax2.bar(range(len(class_weights_np)), class_weights_np, color=\"orange\", alpha=0.7)\n",
        "ax2.set_xlabel(\"Age Group\"); ax2.set_ylabel(\"Class Weight\")\n",
        "ax2.set_title(\"Computed Class Weights\")\n",
        "ax2.set_xticks(range(len(dataset.le.classes_)))\n",
        "ax2.set_xticklabels([f'Age_{c}' for c in dataset.le.classes_], rotation=45)\n",
        "ax2.grid(axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"class_imbalance_analysis.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úÖ Class imbalance analysis saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Sample ECG signals with predictions\n",
        "plt.figure(figsize=(15, 10))\n",
        "sample_indices = np.random.choice(len(test_dataset), 6, replace=False)\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    xb, yb = test_dataset[idx]\n",
        "    xb_numpy = xb.cpu().numpy()  # Keep numpy version for plotting\n",
        "    y_true_sample = dataset.le.inverse_transform([yb.item()])[0]\n",
        "    with torch.no_grad():\n",
        "        # Use the original tensor for model inference\n",
        "        out = model(xb.unsqueeze(0).to(DEVICE))\n",
        "        pred_label = dataset.le.inverse_transform([out.argmax(1).item()])[0]\n",
        "        confidence = torch.softmax(out, dim=1).max().item()\n",
        "    \n",
        "    plt.subplot(3, 2, i+1)\n",
        "    plt.plot(xb_numpy.T, alpha=0.7)  # plot all channels\n",
        "    plt.title(f\"True: Age_{y_true_sample} | Pred: Age_{pred_label} | Conf: {confidence:.3f}\")\n",
        "    plt.xlabel(\"Time\"); plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"sample_ecg_predictions.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úÖ Sample ECG predictions saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Model architecture visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.text(0.1, 0.9, \"Advanced CNN Architecture:\", fontsize=16, fontweight='bold')\n",
        "plt.text(0.1, 0.8, \"‚Ä¢ Residual Blocks with Skip Connections\", fontsize=12)\n",
        "plt.text(0.1, 0.75, \"‚Ä¢ Attention Mechanisms for Feature Selection\", fontsize=12)\n",
        "plt.text(0.1, 0.7, \"‚Ä¢ Batch Normalization and Dropout\", fontsize=12)\n",
        "plt.text(0.1, 0.65, \"‚Ä¢ Focal Loss for Class Imbalance\", fontsize=12)\n",
        "plt.text(0.1, 0.6, \"‚Ä¢ Weighted Random Sampling\", fontsize=12)\n",
        "plt.text(0.1, 0.55, \"‚Ä¢ Learning Rate Scheduling\", fontsize=12)\n",
        "plt.text(0.1, 0.5, \"‚Ä¢ Early Stopping\", fontsize=12)\n",
        "plt.text(0.1, 0.4, f\"Final Performance:\", fontsize=14, fontweight='bold')\n",
        "plt.text(0.1, 0.35, f\"‚Ä¢ Accuracy: {acc:.4f}\", fontsize=12)\n",
        "plt.text(0.1, 0.3, f\"‚Ä¢ F1-Macro: {f1_macro:.4f}\", fontsize=12)\n",
        "plt.text(0.1, 0.25, f\"‚Ä¢ F1-Weighted: {f1_weighted:.4f}\", fontsize=12)\n",
        "plt.text(0.1, 0.2, f\"‚Ä¢ Classes: {n_classes}\", fontsize=12)\n",
        "plt.text(0.1, 0.15, f\"‚Ä¢ Channels: {n_channels}\", fontsize=12)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"model_summary.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úÖ Model summary saved\")\n",
        "\n",
        "# Final summary\n",
        "print(\"\\nüéâ CNN training completed!\")\n",
        "print(f\"üìÅ Results saved to: {OUTPUT_DIR}/\")\n",
        "print(\"üìä Generated files:\")\n",
        "print(\"  ‚Ä¢ training_analysis.png\")\n",
        "print(\"  ‚Ä¢ confusion_matrix.png\")\n",
        "print(\"  ‚Ä¢ per_class_performance.png\")\n",
        "print(\"  ‚Ä¢ class_imbalance_analysis.png\")\n",
        "print(\"  ‚Ä¢ sample_ecg_predictions.png\")\n",
        "print(\"  ‚Ä¢ model_summary.png\")\n",
        "print(\"  ‚Ä¢ best_model.pth\")\n",
        "\n",
        "print(f\"\\nüìà Final Metrics:\")\n",
        "print(f\"  ‚Ä¢ Test Accuracy: {acc:.4f}\")\n",
        "print(f\"  ‚Ä¢ F1-Score (Macro): {f1_macro:.4f}\")\n",
        "print(f\"  ‚Ä¢ F1-Score (Weighted): {f1_weighted:.4f}\")\n",
        "print(f\"  ‚Ä¢ Training time: {training_time:.2f} seconds\")\n",
        "print(f\"  ‚Ä¢ Total parameters: {total_params:,}\")\n",
        "print(f\"  ‚Ä¢ Best validation accuracy: {best_val_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
