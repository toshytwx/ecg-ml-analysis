{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECG Linear Classification Training\n",
        "\n",
        "This notebook implements logistic regression for ECG age group classification with:\n",
        "\n",
        "- **Feature selection**: SelectKBest with ANOVA F-test\n",
        "- **Class imbalance handling**: SMOTE oversampling\n",
        "- **Hyperparameter tuning**: GridSearchCV\n",
        "- **Comprehensive evaluation**: Multiple metrics and visualizations\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Loading](#data-loading)\n",
        "2. [Data Preprocessing](#data-preprocessing)\n",
        "3. [Feature Selection](#feature-selection)\n",
        "4. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
        "5. [Model Training](#model-training)\n",
        "6. [Model Evaluation](#model-evaluation)\n",
        "7. [Visualization and Analysis](#visualization-and-analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load ECG features from CSV and set up the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Environment setup completed\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")  # keep output tidy; remove if debugging\n",
        "\n",
        "# Configuration\n",
        "CSV_FILE = \"../output/ecg_features.csv\"   # change if needed\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "OUTPUT_DIR = \"./ecg_linear_outputs\"\n",
        "OUTPUT_PNG = os.path.join(OUTPUT_DIR, \"all_plots.png\")\n",
        "K_FEATURES = 20   # choose at least 10 features (user requested min 10)\n",
        "SMOTE_K_NEIGHBORS = 3\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"✅ Environment setup completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading: ./output/ecg_features.csv\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './output/ecg_features.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading:\u001b[39m\u001b[38;5;124m\"\u001b[39m, CSV_FILE)\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# If 'record' column exists but isn't needed, keep it aside (not used as feature)\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './output/ecg_features.csv'"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "print(\"Loading:\", CSV_FILE)\n",
        "data = pd.read_csv(CSV_FILE)\n",
        "print(\"Shape:\", data.shape)\n",
        "\n",
        "# If 'record' column exists but isn't needed, keep it aside (not used as feature)\n",
        "if \"record\" in data.columns:\n",
        "    record_col = data[\"record\"]\n",
        "else:\n",
        "    record_col = None\n",
        "\n",
        "print(\"✅ Data loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Handle missing values and filter features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess missing values\n",
        "# Fill missing values with the least common (mode-like but least frequent) within column as in baseline\n",
        "from collections import Counter as _Counter\n",
        "for col in data.columns:\n",
        "    if data[col].isnull().any():\n",
        "        counts = _Counter(data[col].dropna())\n",
        "        if counts:\n",
        "            least_common_value = counts.most_common()[-1][0]\n",
        "            data[col].fillna(least_common_value, inplace=True)\n",
        "\n",
        "# Feature filtering\n",
        "# Remove explicit non-feature columns if present\n",
        "drop_cols = {\"record\", \"age_group\", \"gender\", \"device\"}\n",
        "available_cols = [c for c in data.columns if c not in drop_cols]\n",
        "\n",
        "# Drop columns with more than 30% missing values (fallback; should be none after fill, but keep for safety)\n",
        "threshold = 0.3 * len(data)\n",
        "valid_features = [c for c in available_cols if data[c].count() >= threshold]\n",
        "\n",
        "# Remove features with extremely small magnitude (mean absolute < 1e-6)\n",
        "filtered_features = [c for c in valid_features if np.nanmean(np.abs(data[c])) > 1e-6]\n",
        "\n",
        "# Remove zero-variance features\n",
        "filtered_features = [c for c in filtered_features if data[c].nunique() > 1]\n",
        "\n",
        "print(f\"Candidate features after filtering: {len(filtered_features)}\")\n",
        "print(filtered_features)\n",
        "\n",
        "# Target\n",
        "if \"age_group\" not in data.columns:\n",
        "    raise RuntimeError(\"CSV must contain 'age_group' column\")\n",
        "y = data[\"age_group\"].astype(int)\n",
        "\n",
        "print(\"✅ Data preprocessing completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Selection\n",
        "\n",
        "Select top-K features using SelectKBest with ANOVA F-test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select top-K features\n",
        "X_all = data[filtered_features].copy()\n",
        "\n",
        "# If we have fewer than K_FEATURES available, reduce K accordingly\n",
        "K = min(K_FEATURES, X_all.shape[1])\n",
        "if K < 10:\n",
        "    print(\"Warning: fewer than 10 valid features available, using all available features:\", X_all.shape[1])\n",
        "\n",
        "# Use SelectKBest (ANOVA F-test) to choose features predictive of multiclass target\n",
        "selector = SelectKBest(score_func=f_classif, k=K)\n",
        "# Fill any leftover NaNs (shouldn't exist) with column median\n",
        "X_all = X_all.fillna(X_all.median())\n",
        "selector.fit(X_all, y)\n",
        "selected_mask = selector.get_support()\n",
        "selected_features = X_all.columns[selected_mask].tolist()\n",
        "\n",
        "print(f\"Selected top-{K} features ({len(selected_features)}):\")\n",
        "print(selected_features)\n",
        "\n",
        "X = X_all[selected_features].copy()\n",
        "\n",
        "print(\"✅ Feature selection completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Tuning\n",
        "\n",
        "Set up train/test split, SMOTE, and GridSearchCV for hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train / Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
        "print(\"Class distribution (train):\", np.bincount(y_train))\n",
        "print(\"Class distribution (test):\", np.bincount(y_test))\n",
        "\n",
        "# Handle class imbalance (SMOTE)\n",
        "sm = SMOTE(random_state=RANDOM_STATE, k_neighbors=min(SMOTE_K_NEIGHBORS, max(1, int(np.min(np.bincount(y_train)) - 1))))\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Class distribution before SMOTE:\", np.bincount(y_train))\n",
        "print(\"Class distribution after SMOTE:\", np.bincount(y_train_res))\n",
        "\n",
        "# Feature scaling\n",
        "scaler = RobustScaler()\n",
        "X_train_res_scaled = scaler.fit_transform(X_train_res)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Remove any columns that became constant or inf/nan after scaling\n",
        "def clean_numeric_arrays(X_arr, feature_names):\n",
        "    # X_arr: numpy array\n",
        "    # feature_names: list\n",
        "    mask_ok = np.isfinite(X_arr).all(axis=0) & (np.nanstd(X_arr, axis=0) > 0)\n",
        "    if not mask_ok.all():\n",
        "        keep = [f for f, ok in zip(feature_names, mask_ok) if ok]\n",
        "        X_arr = X_arr[:, mask_ok]\n",
        "        print(\"Dropped features due to NaN/Inf/zero-variance after scaling:\", [f for f in feature_names if f not in keep])\n",
        "        feature_names = keep\n",
        "    return X_arr, feature_names\n",
        "\n",
        "X_train_res_scaled, selected_features = clean_numeric_arrays(X_train_res_scaled, selected_features)\n",
        "X_test_scaled, selected_features = clean_numeric_arrays(X_test_scaled, selected_features)\n",
        "\n",
        "print(\"✅ Data preparation completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid Search for Logistic Regression\n",
        "# Build a logistic regression that's suitable for multiclass:\n",
        "# allow both 'liblinear' (ovr) and 'saga' (multinomial option) in grid\n",
        "param_grid = {\n",
        "    \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
        "    \"penalty\": [\"l1\", \"l2\"],\n",
        "    \"solver\": [\"liblinear\", \"saga\"],\n",
        "    \"class_weight\": [\"balanced\"]\n",
        "}\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=20000, random_state=RANDOM_STATE)\n",
        "grid = GridSearchCV(log_reg, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train_res_scaled, y_train_res)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best CV Score:\", grid.best_score_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "print(\"✅ Hyperparameter tuning completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n",
        "\n",
        "Train the best model and evaluate performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, digits=4, zero_division=0))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"✅ Model training and evaluation completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Evaluation\n",
        "\n",
        "Generate comprehensive visualizations and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({\"figure.max_open_warning\": 0})\n",
        "\n",
        "# 1) Class distribution before / after SMOTE\n",
        "fig1, ax1 = plt.subplots(1, 2, figsize=(12, 4))\n",
        "train_counts = Counter(y_train)\n",
        "res_counts = Counter(y_train_res)\n",
        "ax1[0].bar(list(train_counts.keys()), list(train_counts.values()))\n",
        "ax1[0].set_title(\"Train class distribution (before SMOTE)\")\n",
        "ax1[0].set_xlabel(\"age_group\")\n",
        "ax1[0].set_ylabel(\"count\")\n",
        "\n",
        "ax1[1].bar(list(res_counts.keys()), list(res_counts.values()))\n",
        "ax1[1].set_title(\"Train class distribution (after SMOTE)\")\n",
        "ax1[1].set_xlabel(\"age_group\")\n",
        "plt.tight_layout()\n",
        "fig1.savefig(os.path.join(OUTPUT_DIR, \"class_distribution.png\"))\n",
        "\n",
        "# 2) Correlation heatmap for selected features\n",
        "fig2, ax2 = plt.subplots(1, 1, figsize=(10, 8))\n",
        "corr = pd.DataFrame(X_train_res, columns=selected_features).corr()\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", square=True, ax=ax2)\n",
        "ax2.set_title(\"Feature correlation (train, selected features)\")\n",
        "plt.tight_layout()\n",
        "fig2.savefig(os.path.join(OUTPUT_DIR, \"feature_correlation.png\"))\n",
        "\n",
        "print(\"✅ Class distribution and correlation plots saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Boxplots/violinplots of top 4 features by f-score (SelectKBest scores)\n",
        "# compute column scores from selector (we have selector.scores_ that corresponds to X_all.columns)\n",
        "# build DataFrame with selected features and y (original)\n",
        "df_for_plots = pd.concat([X[selected_features].reset_index(drop=True), y.reset_index(drop=True)], axis=1)\n",
        "top4 = selected_features[:4]  # good heuristic\n",
        "fig3, axes3 = plt.subplots(2, 2, figsize=(12, 8))\n",
        "axes3 = axes3.flatten()\n",
        "for ax, col in zip(axes3, top4):\n",
        "    sns.boxplot(x=\"age_group\", y=col, data=df_for_plots, ax=ax)\n",
        "    ax.set_title(f\"{col} by age_group\")\n",
        "plt.tight_layout()\n",
        "fig3.savefig(os.path.join(OUTPUT_DIR, \"boxplots_top4.png\"))\n",
        "\n",
        "# 4) Learning curve (train vs cross-val)\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    best_model, np.vstack([X_train_res_scaled, X_test_scaled])[:len(X_train_res_scaled)], np.concatenate([y_train_res, y_test])[:len(y_train_res)],\n",
        "    cv=3, scoring=\"accuracy\", train_sizes=np.linspace(0.1, 1.0, 5), n_jobs=-1, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "\n",
        "fig4, ax4 = plt.subplots(figsize=(8, 6))\n",
        "ax4.plot(train_sizes, train_scores_mean, marker=\"o\", label=\"Training score\")\n",
        "ax4.plot(train_sizes, test_scores_mean, marker=\"o\", label=\"Cross-validation score\")\n",
        "ax4.set_xlabel(\"Number of training examples\")\n",
        "ax4.set_ylabel(\"Score (accuracy)\")\n",
        "ax4.set_title(\"Learning Curve\")\n",
        "ax4.legend()\n",
        "ax4.grid(True)\n",
        "fig4.savefig(os.path.join(OUTPUT_DIR, \"learning_curve.png\"))\n",
        "\n",
        "print(\"✅ Boxplots and learning curve saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) GridSearch CV results: mean_test_score vs C, separated by penalty\n",
        "results = pd.DataFrame(grid.cv_results_)\n",
        "fig5, ax5 = plt.subplots(figsize=(8, 6))\n",
        "for penalty in results[\"param_penalty\"].unique():\n",
        "    subset = results[results[\"param_penalty\"] == penalty]\n",
        "    # some solvers may be incompatible and produce NaN; drop NaNs\n",
        "    subset = subset.dropna(subset=[\"param_C\", \"mean_test_score\"])\n",
        "    ax5.plot(subset[\"param_C\"].astype(float), subset[\"mean_test_score\"], marker=\"o\", label=f\"penalty={penalty}\")\n",
        "ax5.set_xscale(\"log\")\n",
        "ax5.set_xlabel(\"C (regularization inverse strength)\")\n",
        "ax5.set_ylabel(\"Mean CV accuracy\")\n",
        "ax5.set_title(\"GridSearchCV results by penalty\")\n",
        "ax5.legend()\n",
        "ax5.grid(True)\n",
        "fig5.savefig(os.path.join(OUTPUT_DIR, \"gridsearch_results.png\"))\n",
        "\n",
        "# 6) Confusion matrix heatmap\n",
        "fig6, ax6 = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax6)\n",
        "ax6.set_xlabel(\"Predicted age_group\")\n",
        "ax6.set_ylabel(\"True age_group\")\n",
        "ax6.set_title(f\"Confusion Matrix (Test Accuracy: {acc:.3f})\")\n",
        "fig6.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"))\n",
        "\n",
        "print(\"✅ GridSearch and confusion matrix plots saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) Feature importance: mean absolute coefficient across classes (for multiclass LR)\n",
        "coefs = best_model.coef_\n",
        "# if binary, coefs is shape (1, n_features), else (n_classes, n_features)\n",
        "mean_abs_coef = np.mean(np.abs(coefs), axis=0)\n",
        "feat_imp = pd.Series(mean_abs_coef, index=selected_features).sort_values(ascending=False)\n",
        "\n",
        "fig7, ax7 = plt.subplots(figsize=(10, 6))\n",
        "sns.barplot(x=feat_imp.values, y=feat_imp.index, ax=ax7)\n",
        "ax7.set_title(\"Feature importance (mean abs coef across classes)\")\n",
        "ax7.set_xlabel(\"Mean |coefficient|\")\n",
        "plt.tight_layout()\n",
        "fig7.savefig(os.path.join(OUTPUT_DIR, \"feature_importance.png\"))\n",
        "\n",
        "print(\"✅ Feature importance plot saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization and Analysis\n",
        "\n",
        "Create combined summary plot and save model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compose a combined figure with some of these plots for convenience\n",
        "fig_combined, axs = plt.subplots(3, 2, figsize=(14, 18))\n",
        "axs = axs.flatten()\n",
        "\n",
        "# 0: class dist before\n",
        "sns.barplot(x=list(train_counts.keys()), y=list(train_counts.values()), ax=axs[0])\n",
        "axs[0].set_title(\"Train distribution (before SMOTE)\")\n",
        "\n",
        "# 1: class dist after\n",
        "sns.barplot(x=list(res_counts.keys()), y=list(res_counts.values()), ax=axs[1])\n",
        "axs[1].set_title(\"Train distribution (after SMOTE)\")\n",
        "\n",
        "# 2: correlation heatmap small version\n",
        "sns.heatmap(corr, annot=False, ax=axs[2])\n",
        "axs[2].set_title(\"Feature correlation (overview)\")\n",
        "\n",
        "# 3: learning curve\n",
        "axs[3].plot(train_sizes, train_scores_mean, marker=\"o\", label=\"Train\")\n",
        "axs[3].plot(train_sizes, test_scores_mean, marker=\"o\", label=\"CV\")\n",
        "axs[3].set_title(\"Learning curve\")\n",
        "axs[3].legend()\n",
        "axs[3].grid(True)\n",
        "\n",
        "# 4: confusion matrix\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", ax=axs[4])\n",
        "axs[4].set_title(\"Confusion matrix\")\n",
        "\n",
        "# 5: top features bar\n",
        "sns.barplot(x=feat_imp.values[:8], y=feat_imp.index[:8], ax=axs[5])\n",
        "axs[5].set_title(\"Top features (mean abs coef)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "fig_combined.savefig(OUTPUT_PNG)\n",
        "\n",
        "print(\"✅ Combined summary plot saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save selected features and model info for reproducibility\n",
        "meta = {\n",
        "    \"selected_features\": selected_features,\n",
        "    \"best_params\": grid.best_params_,\n",
        "    \"best_cv_score\": float(grid.best_score_),\n",
        "    \"test_accuracy\": float(acc)\n",
        "}\n",
        "pd.Series(meta).to_json(os.path.join(OUTPUT_DIR, \"model_meta.json\"))\n",
        "\n",
        "# Save model using joblib (optional)\n",
        "try:\n",
        "    import joblib\n",
        "    MODEL_PATH = os.path.join(OUTPUT_DIR, \"logreg_model.joblib\")\n",
        "    joblib.dump({\"model\": best_model, \"scaler\": scaler, \"features\": selected_features}, MODEL_PATH)\n",
        "    print(\"Saved model to\", MODEL_PATH)\n",
        "except Exception as e:\n",
        "    print(\"joblib save skipped:\", e)\n",
        "\n",
        "print(\"Plots saved to:\", OUTPUT_DIR)\n",
        "print(\"Combined PNG:\", OUTPUT_PNG)\n",
        "\n",
        "print(f\"\\n🎉 Linear classification training completed!\")\n",
        "print(f\"📁 Results saved to: {OUTPUT_DIR}/\")\n",
        "print(\"📊 Generated files:\")\n",
        "print(\"  • class_distribution.png\")\n",
        "print(\"  • feature_correlation.png\")\n",
        "print(\"  • boxplots_top4.png\")\n",
        "print(\"  • learning_curve.png\")\n",
        "print(\"  • gridsearch_results.png\")\n",
        "print(\"  • confusion_matrix.png\")\n",
        "print(\"  • feature_importance.png\")\n",
        "print(\"  • all_plots.png\")\n",
        "print(\"  • model_meta.json\")\n",
        "print(\"  • logreg_model.joblib\")\n",
        "\n",
        "print(f\"\\n📈 Final Metrics:\")\n",
        "print(f\"  • Test Accuracy: {acc:.4f}\")\n",
        "print(f\"  • Best CV Score: {grid.best_score_:.4f}\")\n",
        "print(f\"  • Best Parameters: {grid.best_params_}\")\n",
        "print(f\"  • Selected Features: {len(selected_features)}\")\n",
        "print(\"Done.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
