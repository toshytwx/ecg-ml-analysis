{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECG Autoencoder Training\n",
        "\n",
        "This notebook implements a 1D Convolutional Autoencoder for ECG data analysis with the following capabilities:\n",
        "\n",
        "- **Encoder**: Compresses ECG signals into latent representations\n",
        "- **Decoder**: Reconstructs signals from latent space  \n",
        "- **Applications**: Denoising, feature learning, data augmentation\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Loading](#data-loading)\n",
        "2. [Data Preprocessing](#data-preprocessing)\n",
        "3. [Model Architecture](#model-architecture)\n",
        "4. [Hyperparameter Configuration](#hyperparameter-configuration)\n",
        "5. [Model Training](#model-training)\n",
        "6. [Model Evaluation](#model-evaluation)\n",
        "7. [Visualization and Analysis](#visualization-and-analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "First, we'll set up the environment and load the ECG data from WFDB files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import wfdb\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up device\n",
        "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = \"../input/autonomic-aging-a-dataset-to-quantify-changes-of-cardiovascular-autonomic-function-during-healthy-aging-1.0.0\"\n",
        "OUTPUT_DIR = \"ecg_autoencoder_outputs\"\n",
        "SIGNAL_LENGTH = 2500  # 10 seconds at 250 Hz\n",
        "SIGNAL_DOWNSAMPLE = 1  # Keep original sampling rate\n",
        "PRELOAD_DATA = True\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 0  # Disable multiprocessing for macOS compatibility\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Loading ECG data...\n",
            "Loaded subject info: 1121 subjects\n",
            "Age groups available: [ 2.  7.  4.  3.  1.  9.  8. nan 12.  5. 10. 11.  6. 13. 15. 14.]\n",
            "Found 1120 ECG files\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading signals: 100%|██████████| 1120/1120 [00:21<00:00, 52.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1120 signals\n",
            "Signal shape: (1120, 1, 2500)\n",
            "Age groups: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
            "Age distribution: [  0  46 422 254 105  43  50  50  50  18  24  19  13   7  12   7]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load ECG data function\n",
        "def load_ecg_data():\n",
        "    \"\"\"Load and preprocess ECG data for autoencoder training\"\"\"\n",
        "    print(\"🔄 Loading ECG data...\")\n",
        "    \n",
        "    # Load subject information from CSV\n",
        "    subject_info_csv = os.path.join(DATA_DIR, \"subject-info.csv\")\n",
        "    if not os.path.exists(subject_info_csv):\n",
        "        print(f\"❌ Subject info CSV not found: {subject_info_csv}\")\n",
        "        return None, None\n",
        "    \n",
        "    subject_info = pd.read_csv(subject_info_csv, index_col=0)\n",
        "    print(f\"Loaded subject info: {len(subject_info)} subjects\")\n",
        "    print(f\"Age groups available: {subject_info['Age_group'].unique()}\")\n",
        "    \n",
        "    # Get all .dat files\n",
        "    dat_files = list(Path(DATA_DIR).glob(\"*.dat\"))\n",
        "    print(f\"Found {len(dat_files)} ECG files\")\n",
        "    \n",
        "    signals = []\n",
        "    ages = []\n",
        "    \n",
        "    for i, dat_file in enumerate(tqdm(dat_files, desc=\"Loading signals\")):\n",
        "        try:\n",
        "            # Read signal\n",
        "            record_name = dat_file.stem\n",
        "            signal, fields = wfdb.rdsamp(str(dat_file.parent / record_name))\n",
        "            \n",
        "            # Use first lead (Lead I)\n",
        "            if signal.ndim > 1:\n",
        "                signal = signal[:, 0]\n",
        "            \n",
        "            # Downsample if needed\n",
        "            if SIGNAL_DOWNSAMPLE > 1:\n",
        "                signal = signal[::SIGNAL_DOWNSAMPLE]\n",
        "            \n",
        "            # Pad or truncate to fixed length\n",
        "            if len(signal) > SIGNAL_LENGTH:\n",
        "                signal = signal[:SIGNAL_LENGTH]\n",
        "            else:\n",
        "                signal = np.pad(signal, (0, SIGNAL_LENGTH - len(signal)), 'constant')\n",
        "            \n",
        "            # Normalize signal\n",
        "            signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-8)\n",
        "            signal = signal.astype(np.float32)\n",
        "            \n",
        "            signals.append(signal)\n",
        "            \n",
        "            # Get age from subject info CSV\n",
        "            try:\n",
        "                record_id = int(record_name)\n",
        "                if record_id in subject_info.index:\n",
        "                    age_group = subject_info.loc[record_id, 'Age_group']\n",
        "                    # Handle NaN values\n",
        "                    if pd.isna(age_group):\n",
        "                        ages.append(3)  # Default age group for NaN\n",
        "                    else:\n",
        "                        ages.append(int(age_group))\n",
        "                else:\n",
        "                    print(f\"Warning: Record {record_id} not found in subject info\")\n",
        "                    ages.append(3)  # Default age group\n",
        "            except:\n",
        "                ages.append(3)  # Default age group if parsing fails\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {dat_file}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    signals = np.array(signals)\n",
        "    ages = np.array(ages)\n",
        "    \n",
        "    # Reshape signals to have channel dimension\n",
        "    signals = signals.reshape(signals.shape[0], 1, signals.shape[1])  # (N, 1, 2500)\n",
        "    \n",
        "    print(f\"Loaded {len(signals)} signals\")\n",
        "    print(f\"Signal shape: {signals.shape}\")\n",
        "    print(f\"Age groups: {np.unique(ages)}\")\n",
        "    print(f\"Age distribution: {np.bincount(ages)}\")\n",
        "    \n",
        "    return signals, ages\n",
        "\n",
        "# Load the data\n",
        "signals, ages = load_ecg_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Now we'll create age groups and prepare the data for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 896\n",
            "Validation samples: 224\n",
            "Age group distribution in training: [896]\n",
            "Age group distribution in validation: [224]\n"
          ]
        }
      ],
      "source": [
        "# Create age groups for analysis\n",
        "def create_age_groups(ages):\n",
        "    \"\"\"Create age groups for analysis\"\"\"\n",
        "    age_groups = []\n",
        "    for age in ages:\n",
        "        if age < 30:\n",
        "            age_groups.append(0)\n",
        "        elif age < 40:\n",
        "            age_groups.append(1)\n",
        "        elif age < 50:\n",
        "            age_groups.append(2)\n",
        "        elif age < 60:\n",
        "            age_groups.append(3)\n",
        "        elif age < 70:\n",
        "            age_groups.append(4)\n",
        "        elif age < 80:\n",
        "            age_groups.append(5)\n",
        "        else:\n",
        "            age_groups.append(6)\n",
        "    return np.array(age_groups)\n",
        "\n",
        "# Create age groups\n",
        "age_groups = create_age_groups(ages)\n",
        "\n",
        "# Split data into train/validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    signals, age_groups, test_size=0.2, random_state=42, stratify=age_groups\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "print(f\"Age group distribution in training: {np.bincount(y_train)}\")\n",
        "print(f\"Age group distribution in validation: {np.bincount(y_val)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Architecture\n",
        "\n",
        "Define the 1D Convolutional Autoencoder architecture with encoder and decoder components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Autoencoder model architecture defined\n"
          ]
        }
      ],
      "source": [
        "# Dataset class for autoencoder training\n",
        "class ECGAutoencoderDataset(Dataset):\n",
        "    \"\"\"Dataset for ECG autoencoder training\"\"\"\n",
        "    \n",
        "    def __init__(self, signals, labels=None, transform=None):\n",
        "        self.signals = signals\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.signals)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        signal = self.signals[idx]\n",
        "        \n",
        "        if self.transform:\n",
        "            signal = self.transform(signal)\n",
        "            \n",
        "        if self.labels is not None:\n",
        "            return signal, self.labels[idx]\n",
        "        else:\n",
        "            return signal, signal  # For autoencoder, input = target\n",
        "\n",
        "# Autoencoder model\n",
        "class ECGAutoencoder(nn.Module):\n",
        "    \"\"\"1D Convolutional Autoencoder for ECG signals\"\"\"\n",
        "    \n",
        "    def __init__(self, input_length, latent_dim, encoder_channels, decoder_channels, \n",
        "                 kernel_size, stride, padding):\n",
        "        super(ECGAutoencoder, self).__init__()\n",
        "        \n",
        "        self.input_length = input_length\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.ModuleList()\n",
        "        current_length = input_length\n",
        "        \n",
        "        for i in range(len(encoder_channels) - 1):\n",
        "            in_channels = encoder_channels[i]\n",
        "            out_channels = encoder_channels[i + 1]\n",
        "            \n",
        "            self.encoder.append(nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                nn.BatchNorm1d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(0.1)\n",
        "            ))\n",
        "            current_length = (current_length + 2 * padding - kernel_size) // stride + 1\n",
        "        \n",
        "        # Latent space projection\n",
        "        self.latent_proj = nn.Linear(encoder_channels[-1] * current_length, latent_dim)\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder_proj = nn.Linear(latent_dim, encoder_channels[-1] * current_length)\n",
        "        \n",
        "        self.decoder = nn.ModuleList()\n",
        "        for i in range(len(decoder_channels) - 1):\n",
        "            in_channels = decoder_channels[i]\n",
        "            out_channels = decoder_channels[i + 1]\n",
        "            \n",
        "            self.decoder.append(nn.Sequential(\n",
        "                nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                nn.BatchNorm1d(out_channels),\n",
        "                nn.ReLU(inplace=True) if i < len(decoder_channels) - 2 else nn.Identity()\n",
        "            ))\n",
        "        \n",
        "        # Store dimensions for reconstruction\n",
        "        self.encoder_channels = encoder_channels\n",
        "        self.decoder_channels = decoder_channels\n",
        "        self.current_length = current_length\n",
        "        \n",
        "    def encode(self, x):\n",
        "        \"\"\"Encode input to latent space\"\"\"\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "        \n",
        "        # Flatten and project to latent space\n",
        "        x = x.view(x.size(0), -1)\n",
        "        z = self.latent_proj(x)\n",
        "        return z\n",
        "    \n",
        "    def decode(self, z):\n",
        "        \"\"\"Decode latent representation to output\"\"\"\n",
        "        # Project back to encoder output shape\n",
        "        x = self.decoder_proj(z)\n",
        "        x = x.view(x.size(0), self.encoder_channels[-1], self.current_length)\n",
        "        \n",
        "        # Decode through transposed convolutions\n",
        "        for layer in self.decoder:\n",
        "            x = layer(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        z = self.encode(x)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, z\n",
        "\n",
        "print(\"✅ Autoencoder model architecture defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Configuration\n",
        "\n",
        "Set up the model hyperparameters and training configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 Hyperparameter Configuration:\n",
            "  • Latent dimension: 64\n",
            "  • Encoder channels: [1, 32, 64, 128]\n",
            "  • Decoder channels: [128, 64, 32, 1]\n",
            "  • Kernel size: 15\n",
            "  • Learning rate: 0.001\n",
            "  • Epochs: 50\n",
            "  • Batch size: 64\n",
            "  • MSE weight: 1.0\n",
            "  • L1 weight: 0.1\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "LATENT_DIM = 64  # Latent space dimension\n",
        "ENCODER_CHANNELS = [1, 32, 64, 128]  # Encoder channel progression\n",
        "DECODER_CHANNELS = [128, 64, 32, 1]  # Decoder channel progression\n",
        "KERNEL_SIZE = 15\n",
        "STRIDE = 1\n",
        "PADDING = 7\n",
        "\n",
        "# Training configuration\n",
        "EPOCHS = 50\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "PATIENCE = 10\n",
        "MIN_DELTA = 1e-4\n",
        "\n",
        "# Loss configuration\n",
        "MSE_WEIGHT = 1.0\n",
        "L1_WEIGHT = 0.1\n",
        "\n",
        "print(\"📋 Hyperparameter Configuration:\")\n",
        "print(f\"  • Latent dimension: {LATENT_DIM}\")\n",
        "print(f\"  • Encoder channels: {ENCODER_CHANNELS}\")\n",
        "print(f\"  • Decoder channels: {DECODER_CHANNELS}\")\n",
        "print(f\"  • Kernel size: {KERNEL_SIZE}\")\n",
        "print(f\"  • Learning rate: {LR}\")\n",
        "print(f\"  • Epochs: {EPOCHS}\")\n",
        "print(f\"  • Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  • MSE weight: {MSE_WEIGHT}\")\n",
        "print(f\"  • L1 weight: {L1_WEIGHT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n",
        "\n",
        "Initialize the model, create data loaders, and train the autoencoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏗️ Model Architecture:\n",
            "  • Total parameters: 41,589,187\n",
            "  • Trainable parameters: 41,589,187\n",
            "  • Model size: 158.65 MB\n"
          ]
        }
      ],
      "source": [
        "# Create datasets\n",
        "train_dataset = ECGAutoencoderDataset(X_train)\n",
        "val_dataset = ECGAutoencoderDataset(X_val)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False,\n",
        "    persistent_workers=False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False,\n",
        "    persistent_workers=False\n",
        ")\n",
        "\n",
        "# Create model\n",
        "model = ECGAutoencoder(\n",
        "    input_length=SIGNAL_LENGTH,\n",
        "    latent_dim=LATENT_DIM,\n",
        "    encoder_channels=ENCODER_CHANNELS,\n",
        "    decoder_channels=DECODER_CHANNELS,\n",
        "    kernel_size=KERNEL_SIZE,\n",
        "    stride=STRIDE,\n",
        "    padding=PADDING\n",
        ").to(DEVICE)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"🏗️ Model Architecture:\")\n",
        "print(f\"  • Total parameters: {total_params:,}\")\n",
        "print(f\"  • Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  • Model size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training functions\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device, epoch):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    mse_loss = 0\n",
        "    l1_loss = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False)\n",
        "    for batch_idx, (data, _) in enumerate(pbar):\n",
        "        data = data.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        recon_data, latent = model(data)\n",
        "        \n",
        "        # Calculate losses\n",
        "        mse = F.mse_loss(recon_data, data)\n",
        "        l1 = F.l1_loss(recon_data, data)\n",
        "        \n",
        "        loss = MSE_WEIGHT * mse + L1_WEIGHT * l1\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        mse_loss += mse.item()\n",
        "        l1_loss += l1.item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'MSE': f'{mse.item():.4f}',\n",
        "            'L1': f'{l1.item():.4f}'\n",
        "        })\n",
        "        \n",
        "        # Memory management\n",
        "        if batch_idx % 100 == 0:\n",
        "            if device.type == 'mps':\n",
        "                torch.mps.empty_cache()\n",
        "    \n",
        "    return total_loss / len(train_loader), mse_loss / len(train_loader), l1_loss / len(train_loader)\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    mse_loss = 0\n",
        "    l1_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, _ in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "            data = data.to(device)\n",
        "            \n",
        "            recon_data, latent = model(data)\n",
        "            \n",
        "            mse = F.mse_loss(recon_data, data)\n",
        "            l1 = F.l1_loss(recon_data, data)\n",
        "            \n",
        "            loss = MSE_WEIGHT * mse + L1_WEIGHT * l1\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            mse_loss += mse.item()\n",
        "            l1_loss += l1.item()\n",
        "    \n",
        "    return total_loss / len(val_loader), mse_loss / len(val_loader), l1_loss / len(val_loader)\n",
        "\n",
        "print(\"✅ Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize optimizer and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "print(f\"\\n🎯 Training for {EPOCHS} epochs...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Train\n",
        "        train_loss, train_mse, train_l1 = train_epoch(\n",
        "            model, train_loader, optimizer, None, DEVICE, epoch + 1\n",
        "        )\n",
        "        \n",
        "        # Validate\n",
        "        val_loss, val_mse, val_l1 = validate_epoch(\n",
        "            model, val_loader, None, DEVICE\n",
        "        )\n",
        "        \n",
        "        # Update scheduler\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # Store losses\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch + 1:2d}/{EPOCHS}: \"\n",
        "              f\"Train Loss: {train_loss:.4f} (MSE: {train_mse:.4f}, L1: {train_l1:.4f}) | \"\n",
        "              f\"Val Loss: {val_loss:.4f} (MSE: {val_mse:.4f}, L1: {val_l1:.4f})\")\n",
        "        \n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss - MIN_DELTA:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), f\"{OUTPUT_DIR}/best_autoencoder.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            \n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "            \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user\")\n",
        "except Exception as e:\n",
        "    print(f\"Training error: {e}\")\n",
        "finally:\n",
        "    # Cleanup\n",
        "    if DEVICE.type == 'mps':\n",
        "        torch.mps.empty_cache()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\n⏱️  Training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "# Load best model\n",
        "if os.path.exists(f\"{OUTPUT_DIR}/best_autoencoder.pth\"):\n",
        "    model.load_state_dict(torch.load(f\"{OUTPUT_DIR}/best_autoencoder.pth\"))\n",
        "    print(\"✅ Loaded best model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Evaluation\n",
        "\n",
        "Evaluate the trained autoencoder and generate comprehensive visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate training analysis plots\n",
        "def generate_training_analysis(train_losses, val_losses):\n",
        "    \"\"\"Generate training analysis plots\"\"\"\n",
        "    print(\"📊 Generating training analysis...\")\n",
        "    \n",
        "    # Set style\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    # 1. Training curves\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss curves\n",
        "    axes[0, 0].plot(train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
        "    axes[0, 0].plot(val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
        "    axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Loss difference\n",
        "    loss_diff = np.array(val_losses) - np.array(train_losses)\n",
        "    axes[0, 1].plot(loss_diff, color='green', alpha=0.7)\n",
        "    axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "    axes[0, 1].set_title('Overfitting Indicator (Val - Train Loss)', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss Difference')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Learning rate schedule\n",
        "    axes[1, 0].text(0.5, 0.5, 'Learning Rate Schedule\\n(ReduceLROnPlateau)', \n",
        "                   ha='center', va='center', fontsize=12, \n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
        "    axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlim(0, 1)\n",
        "    axes[1, 0].set_ylim(0, 1)\n",
        "    axes[1, 0].axis('off')\n",
        "    \n",
        "    # Model summary\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    # Handle empty loss lists\n",
        "    best_val_loss = min(val_losses) if val_losses else 0.0\n",
        "    final_train_loss = train_losses[-1] if train_losses else 0.0\n",
        "    final_val_loss = val_losses[-1] if val_losses else 0.0\n",
        "    \n",
        "    summary_text = f\"\"\"Model Summary:\n",
        "    \n",
        "Total Parameters: {total_params:,}\n",
        "Trainable Parameters: {trainable_params:,}\n",
        "Latent Dimension: {LATENT_DIM}\n",
        "Input Length: {SIGNAL_LENGTH}\n",
        "Encoder Channels: {ENCODER_CHANNELS}\n",
        "Decoder Channels: {DECODER_CHANNELS}\n",
        "Best Validation Loss: {best_val_loss:.4f}\n",
        "Final Training Loss: {final_train_loss:.4f}\n",
        "Final Validation Loss: {final_val_loss:.4f}\"\"\"\n",
        "    \n",
        "    axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes, \n",
        "                    fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
        "                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\"))\n",
        "    axes[1, 1].set_title('Model Summary', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/autoencoder_training_analysis.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    print(\"✅ Training analysis saved\")\n",
        "\n",
        "# Generate the training analysis\n",
        "generate_training_analysis(train_losses, val_losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate reconstruction examples\n",
        "def generate_reconstruction_examples(model, val_loader, device):\n",
        "    \"\"\"Generate reconstruction examples\"\"\"\n",
        "    print(\"📊 Generating reconstruction examples...\")\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get a batch of validation data\n",
        "        val_batch = next(iter(val_loader))\n",
        "        val_data = val_batch[0][:8].to(device)  # First 8 samples\n",
        "        \n",
        "        # Get reconstructions\n",
        "        recon_data, latent = model(val_data)\n",
        "        \n",
        "        # Convert to numpy\n",
        "        val_data_np = val_data.cpu().numpy()\n",
        "        recon_data_np = recon_data.cpu().numpy()\n",
        "        latent_np = latent.cpu().numpy()\n",
        "    \n",
        "    # Plot reconstruction examples\n",
        "    fig, axes = plt.subplots(4, 2, figsize=(15, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i in range(8):\n",
        "        ax = axes[i]\n",
        "        \n",
        "        # Original signal\n",
        "        ax.plot(val_data_np[i, 0], label='Original', alpha=0.8, linewidth=1.5)\n",
        "        \n",
        "        # Reconstructed signal\n",
        "        ax.plot(recon_data_np[i, 0], label='Reconstructed', alpha=0.8, linewidth=1.5)\n",
        "        \n",
        "        # Calculate reconstruction error\n",
        "        mse = np.mean((val_data_np[i, 0] - recon_data_np[i, 0]) ** 2)\n",
        "        mae = np.mean(np.abs(val_data_np[i, 0] - recon_data_np[i, 0]))\n",
        "        \n",
        "        ax.set_title(f'Sample {i+1} (MSE: {mse:.4f}, MAE: {mae:.4f})', fontsize=10)\n",
        "        ax.set_xlabel('Time')\n",
        "        ax.set_ylabel('Amplitude')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('ECG Signal Reconstruction Examples', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/autoencoder_reconstruction_examples.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    print(\"✅ Reconstruction examples saved\")\n",
        "    return latent_np\n",
        "\n",
        "# Generate reconstruction examples\n",
        "latent_np = generate_reconstruction_examples(model, val_loader, DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate latent space analysis\n",
        "def generate_latent_analysis(latent_np, val_data_np, recon_data_np):\n",
        "    \"\"\"Generate latent space analysis\"\"\"\n",
        "    print(\"📊 Generating latent space analysis...\")\n",
        "    \n",
        "    # 3. Latent space visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Latent space scatter plot (first 2 dimensions)\n",
        "    axes[0, 0].scatter(latent_np[:, 0], latent_np[:, 1], alpha=0.6, s=20)\n",
        "    axes[0, 0].set_title('Latent Space (First 2 Dimensions)', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Latent Dim 1')\n",
        "    axes[0, 0].set_ylabel('Latent Dim 2')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Latent space distribution\n",
        "    axes[0, 1].hist(latent_np.flatten(), bins=50, alpha=0.7, color='skyblue')\n",
        "    axes[0, 1].set_title('Latent Space Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Latent Value')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Reconstruction error distribution\n",
        "    recon_errors = np.mean((val_data_np - recon_data_np) ** 2, axis=(1, 2))\n",
        "    axes[1, 0].hist(recon_errors, bins=30, alpha=0.7, color='lightcoral')\n",
        "    axes[1, 0].set_title('Reconstruction Error Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Mean Squared Error')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Latent space correlation\n",
        "    latent_corr = np.corrcoef(latent_np.T)\n",
        "    im = axes[1, 1].imshow(latent_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    axes[1, 1].set_title('Latent Space Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Latent Dimension')\n",
        "    axes[1, 1].set_ylabel('Latent Dimension')\n",
        "    plt.colorbar(im, ax=axes[1, 1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/autoencoder_latent_analysis.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    print(\"✅ Latent space analysis saved\")\n",
        "\n",
        "# Get validation data for latent analysis\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    val_batch = next(iter(val_loader))\n",
        "    val_data = val_batch[0][:8].to(DEVICE)\n",
        "    recon_data, latent = model(val_data)\n",
        "    val_data_np = val_data.cpu().numpy()\n",
        "    recon_data_np = recon_data.cpu().numpy()\n",
        "\n",
        "# Generate latent space analysis\n",
        "generate_latent_analysis(latent_np, val_data_np, recon_data_np)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization and Analysis\n",
        "\n",
        "Final summary and results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"\\n🎉 Autoencoder training completed!\")\n",
        "print(f\"📁 Results saved to: {OUTPUT_DIR}/\")\n",
        "print(\"📊 Generated files:\")\n",
        "print(\"  • autoencoder_training_analysis.png\")\n",
        "print(\"  • autoencoder_reconstruction_examples.png\") \n",
        "print(\"  • autoencoder_latent_analysis.png\")\n",
        "print(\"  • best_autoencoder.pth\")\n",
        "\n",
        "# Display final metrics\n",
        "if train_losses and val_losses:\n",
        "    print(f\"\\n📈 Final Metrics:\")\n",
        "    print(f\"  • Best validation loss: {min(val_losses):.4f}\")\n",
        "    print(f\"  • Final training loss: {train_losses[-1]:.4f}\")\n",
        "    print(f\"  • Final validation loss: {val_losses[-1]:.4f}\")\n",
        "    print(f\"  • Training time: {training_time:.2f} seconds\")\n",
        "    print(f\"  • Total parameters: {total_params:,}\")\n",
        "    print(f\"  • Latent dimension: {LATENT_DIM}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
